{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1uxo8kTpq3la"
      },
      "source": [
        "# W2NER для русского клинического NER\n",
        "\n",
        "## Извлечение медицинских сущностей из клинических рекомендаций Минздрава РФ\n",
        "\n",
        "Исходник модели тут: https://github.com/ljynlp/W2NER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AH1WE-RHq3lf"
      },
      "source": [
        "\n",
        "---\n",
        "## Setup & Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6e5hNsstO9Sb"
      },
      "outputs": [],
      "source": [
        "import os, sys, glob, zipfile, pathlib, shutil, json, re\n",
        "from types import SimpleNamespace\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "!pip install -q gensim\n",
        "\n",
        "import numpy as np, pandas as pd\n",
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "CONTENT = \"/content\"\n",
        "REPO_ZIP = f\"{CONTENT}/W2NER-main.zip\"\n",
        "REPO = f\"{CONTENT}/W2NER-main\"\n",
        "DATA_GUIDE = f\"{REPO}/data/guidelines_ru\"\n",
        "DATA_PRED = f\"{REPO}/data/predict\"\n",
        "MODELS_DIR = f\"{REPO}/models\"\n",
        "OUTPUTS_DIR = f\"{REPO}/outputs\"\n",
        "CONFIG_PATH = f\"{REPO}/config/guidelines_ru.json\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkFcE8r5sLg3"
      },
      "source": [
        "---\n",
        "## Unpack"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aLyxqiUA8ouV"
      },
      "outputs": [],
      "source": [
        "# Распаковываем W2NER-main.zip\n",
        "with zipfile.ZipFile(REPO_ZIP, 'r') as z:\n",
        "  z.extractall(CONTENT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t3b9YmAWHBui"
      },
      "outputs": [],
      "source": [
        "# Создаем все необходимые папки\n",
        "for d in [DATA_GUIDE, DATA_PRED, MODELS_DIR, OUTPUTS_DIR, f\"{REPO}/log\"]:\n",
        "  pathlib.Path(d).mkdir(parents=True, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u7RCIheAPfkL"
      },
      "outputs": [],
      "source": [
        "# Patch 1: np.int -> int\n",
        "p = f\"{REPO}/data_loader.py\"\n",
        "txt = open(p, \"r\", encoding=\"utf-8\").read()\n",
        "if \"np.int\" in txt:\n",
        "    txt = txt.replace(\"dtype=np.int\", \"dtype=int\")\n",
        "    open(p, \"w\", encoding=\"utf-8\").write(txt)\n",
        "\n",
        "# Patch 2: transformers.AdamW -> torch.optim.AdamW\n",
        "p = f\"{REPO}/main.py\"\n",
        "txt = open(p, \"r\", encoding=\"utf-8\").read()\n",
        "\n",
        "# Заменяем импорт\n",
        "txt = txt.replace(\"from transformers import AdamW\", \"from torch.optim import AdamW\")\n",
        "txt = txt.replace(\"import transformers\", \"import transformers\\nfrom torch.optim import AdamW\")\n",
        "txt = txt.replace(\"transformers.AdamW\", \"AdamW\")\n",
        "\n",
        "open(p, \"w\", encoding=\"utf-8\").write(txt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3fuE9rTPqk1"
      },
      "source": [
        "\n",
        "---\n",
        "## Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "68ES8bR2Pt-j"
      },
      "outputs": [],
      "source": [
        "# config update\n",
        "if os.path.exists(CONFIG_PATH):\n",
        "    cfg = json.load(open(CONFIG_PATH, \"r\", encoding=\"utf-8\"))\n",
        "else:\n",
        "    base_config = f\"{REPO}/config/conll03.json\"\n",
        "    cfg = json.load(open(base_config, \"r\", encoding=\"utf-8\"))\n",
        "    cfg[\"bert_hid_size\"] = 768\n",
        "    cfg[\"lstm_hid_size\"] = 512\n",
        "    cfg[\"use_bert_last_4_layers\"] = False\n",
        "\n",
        "cfg[\"dataset\"] = \"guidelines_ru\"\n",
        "cfg[\"save_path\"] = f\"{MODELS_DIR}/guidelines_ru_model.pt\"\n",
        "cfg[\"predict_path\"] = f\"{OUTPUTS_DIR}/guidelines_ru_pred.json\"\n",
        "cfg[\"epochs\"] = 10\n",
        "cfg[\"bert_name\"] = \"bert-base-multilingual-cased\"\n",
        "\n",
        "json.dump(cfg, open(CONFIG_PATH, \"w\", encoding=\"utf-8\"), ensure_ascii=False, indent=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CETCG8Pnq3li"
      },
      "source": [
        "\n",
        "---\n",
        "## Data Preparation - Training Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gAy9fblvUibk"
      },
      "outputs": [],
      "source": [
        "# copy training data\n",
        "src_train = \"/content/w2ner_train.json\"\n",
        "src_dev = \"/content/w2ner_dev.json\"\n",
        "src_test = \"/content/w2ner_test.json\"\n",
        "\n",
        "shutil.copy(src_train, f\"{DATA_GUIDE}/train.json\")\n",
        "shutil.copy(src_dev, f\"{DATA_GUIDE}/dev.json\")\n",
        "shutil.copy(src_test, f\"{DATA_GUIDE}/test.json\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LwhDIT2em6xn"
      },
      "outputs": [],
      "source": [
        "os.chdir(REPO)\n",
        "!python main.py --config {CONFIG_PATH}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yn2DsoousZrY"
      },
      "source": [
        "---\n",
        "## Model & Data Copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m4kgzrbcaNc_"
      },
      "outputs": [],
      "source": [
        "all_pt = glob.glob(f\"{REPO}/**/*.pt\", recursive=True)\n",
        "if all_pt:\n",
        "    trained_model = max(all_pt, key=os.path.getsize)\n",
        "    final_path = f\"{MODELS_DIR}/guidelines_ru_model.pt\"\n",
        "    shutil.copy(trained_model, final_path)\n",
        "    model_path = final_path\n",
        "else:\n",
        "    model_path = cfg[\"save_path\"]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwuwM-LfVGhx"
      },
      "source": [
        "---\n",
        "## Подготовка к Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eiTgtfHen3u9"
      },
      "outputs": [],
      "source": [
        "# построение ID2label - словаря сущностей\n",
        "train_path = f\"{DATA_GUIDE}/train.json\"\n",
        "train = json.load(open(train_path, \"r\", encoding=\"utf-8\"))\n",
        "\n",
        "unique_types = set()\n",
        "for record in train:\n",
        "    for entity in record.get('ner', []):\n",
        "        etype = entity.get('type', '')\n",
        "        if etype:\n",
        "            unique_types.add(etype)\n",
        "\n",
        "unique_types = sorted(unique_types)\n",
        "\n",
        "ID2LABEL = {0: \"O\", 1: \"NNW\"}\n",
        "for idx, etype in enumerate(unique_types, start=2):\n",
        "    ID2LABEL[idx] = etype"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkNOBN1_sjUA"
      },
      "source": [
        "---\n",
        "## Model Uploading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e8B5bD4couz1"
      },
      "outputs": [],
      "source": [
        "os.chdir(REPO)\n",
        "sys.path.insert(0, REPO)\n",
        "\n",
        "import data_loader\n",
        "import model as w2model\n",
        "from config import Config\n",
        "import utils\n",
        "\n",
        "args = SimpleNamespace(config=CONFIG_PATH, device=0)\n",
        "cfg = Config(args)\n",
        "cfg.logger = utils.get_logger(cfg.dataset)\n",
        "_ = data_loader.load_data_bert(cfg)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "net = w2model.Model(cfg).to(device)\n",
        "net.load_state_dict(torch.load(model_path, map_location=\"cpu\"), strict=True)\n",
        "net.eval()\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(cfg.bert_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ch_lPHw5nPId"
      },
      "source": [
        "---\n",
        "## Загрузка тестовых данных (MD файлы)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hsJIbIjfbVOd"
      },
      "outputs": [],
      "source": [
        "TEST_ZIP = f\"{CONTENT}/minzdrav_test.zip\"\n",
        "TEST_DIR = f\"{CONTENT}/minzdrav_test\"\n",
        "\n",
        "if os.path.exists(TEST_ZIP):\n",
        "    with zipfile.ZipFile(TEST_ZIP, 'r') as z:\n",
        "        z.extractall(CONTENT)\n",
        "\n",
        "md_files = sorted(glob.glob(f\"{TEST_DIR}/**/*.md\", recursive=True))\n",
        "\n",
        "if not md_files:\n",
        "    md_files = sorted(glob.glob(f\"{CONTENT}/minzdrav_test/**/*.md\", recursive=True))\n",
        "print(f\"Найдено файлов: {len(md_files)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QE_VMOXznW8o"
      },
      "source": [
        "---\n",
        "## Inference (Text processing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MbUf58jLUYj3"
      },
      "outputs": [],
      "source": [
        "MAX_TOKENS = 220\n",
        "MAX_WP = 510\n",
        "\n",
        "def strip_markdown(md: str) -> str:\n",
        "    md = re.sub(r\"```.*?```\", \" \", md, flags=re.S)\n",
        "    md = re.sub(r\"\\|.*?\\|\", \" \", md)\n",
        "    md = re.sub(r\"^#+\\s*\", \"\", md, flags=re.M)\n",
        "    md = re.sub(r\"[*_>`~]\", \" \", md)\n",
        "    md = re.sub(r\"\\s+\", \" \", md)\n",
        "    return md.strip()\n",
        "\n",
        "sent_split = re.compile(r\"(?<=[\\.\\!\\?])\\s+|[\\n\\r]+\")\n",
        "token_pat = re.compile(r\"[A-Za-zА-Яа-яЁё]+|\\d+(?:[.,]\\d+)?|[%°×x/]+|[^\\s]\")\n",
        "\n",
        "def wp_len(tokens):\n",
        "    return len(tokenizer(tokens, is_split_into_words=True, add_special_tokens=True, truncation=False)[\"input_ids\"])\n",
        "\n",
        "def ensure_wp_limit(tokens):\n",
        "    if not tokens:\n",
        "        return []\n",
        "    if wp_len(tokens) <= MAX_WP:\n",
        "        return [tokens]\n",
        "    if len(tokens) <= 1:\n",
        "        return [tokens]\n",
        "    mid = len(tokens) // 2\n",
        "    return ensure_wp_limit(tokens[:mid]) + ensure_wp_limit(tokens[mid:])\n",
        "\n",
        "predict_data_full = []\n",
        "meta_full = []\n",
        "\n",
        "for fp in md_files:\n",
        "    filename = os.path.basename(fp)\n",
        "    raw = open(fp, \"r\", encoding=\"utf-8\", errors=\"ignore\").read()\n",
        "    txt = strip_markdown(raw)\n",
        "    sents = [s.strip() for s in sent_split.split(txt) if s.strip()]\n",
        "\n",
        "    for s in sents:\n",
        "        toks = token_pat.findall(s)\n",
        "        if len(toks) < 3:\n",
        "            continue\n",
        "\n",
        "        token_chunks = []\n",
        "        if len(toks) > MAX_TOKENS:\n",
        "            for k in range(0, len(toks), MAX_TOKENS):\n",
        "                token_chunks.append((k//MAX_TOKENS, toks[k:k+MAX_TOKENS]))\n",
        "        else:\n",
        "            token_chunks.append((0, toks))\n",
        "\n",
        "        for chunk_idx, chunk in token_chunks:\n",
        "            sub_chunks = ensure_wp_limit(chunk)\n",
        "            for sub_idx, sub in enumerate(sub_chunks):\n",
        "                predict_data_full.append({\"sentence\": sub, \"ner\": []})\n",
        "                meta_full.append({\n",
        "                    \"source_file\": filename,\n",
        "                    \"sentence_text\": \" \".join(sub),\n",
        "                    \"orig_sentence\": s,\n",
        "                })\n",
        "\n",
        "# Vocab из train\n",
        "train_data = json.load(open(f\"{DATA_GUIDE}/train.json\", \"r\", encoding=\"utf-8\"))\n",
        "vocab = data_loader.Vocabulary()\n",
        "for record in train_data:\n",
        "    for entity in record.get('ner', []):\n",
        "        label = entity.get('type', '')\n",
        "        if label:\n",
        "            vocab.add_label(label.lower())\n",
        "\n",
        "# DataLoader\n",
        "pred_dataset_full = data_loader.RelationDataset(\n",
        "    *data_loader.process_bert(predict_data_full, tokenizer, vocab)\n",
        ")\n",
        "\n",
        "pred_loader_full = DataLoader(\n",
        "    pred_dataset_full,\n",
        "    batch_size=8,\n",
        "    collate_fn=data_loader.collate_fn,\n",
        "    shuffle=False,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "# Inference\n",
        "all_preds_full = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(pred_loader_full, desc=\"Inference\"):\n",
        "        batch = [x.to(device) if hasattr(x, \"to\") else x for x in batch]\n",
        "\n",
        "        bert_inputs   = batch[0]\n",
        "        grid_mask2d   = batch[2]\n",
        "        pieces2word   = batch[3]\n",
        "        dist_inputs   = batch[4]\n",
        "        sent_length   = batch[5]\n",
        "\n",
        "        outputs = net(bert_inputs, grid_mask2d, dist_inputs, pieces2word, sent_length)\n",
        "\n",
        "        if isinstance(outputs, (tuple, list)):\n",
        "            outputs = outputs[0]\n",
        "\n",
        "        pred_grid = outputs.argmax(dim=-1).detach().cpu().numpy()\n",
        "        all_preds_full.extend(pred_grid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhqorLGkpUkE"
      },
      "source": [
        "---\n",
        "## Decoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kussCNeqpY5E"
      },
      "outputs": [],
      "source": [
        "decoded_full = []\n",
        "total_entities = 0\n",
        "\n",
        "for idx in range(len(all_preds_full)):\n",
        "    pred_grid = all_preds_full[idx]\n",
        "    meta_item = meta_full[idx]\n",
        "    tokens = predict_data_full[idx][\"sentence\"]\n",
        "\n",
        "    entities = []\n",
        "    L = pred_grid.shape[0]\n",
        "\n",
        "    for i in range(L):\n",
        "        for j in range(i, L):\n",
        "            type_id = int(pred_grid[i, j])\n",
        "            if type_id >= 2:\n",
        "                token_indices = list(range(i, j + 1))\n",
        "                if all(idx < len(tokens) for idx in token_indices):\n",
        "                    entity_tokens = [tokens[idx] for idx in token_indices]\n",
        "                    entity_text = \" \".join(entity_tokens)\n",
        "                    entities.append({\n",
        "                        \"type_id\": type_id,\n",
        "                        \"type\": ID2LABEL.get(type_id, f\"UNKNOWN_{type_id}\"),\n",
        "                        \"index\": token_indices,\n",
        "                        \"text\": entity_text\n",
        "                    })\n",
        "\n",
        "    total_entities += len(entities)\n",
        "\n",
        "    decoded_full.append({\n",
        "        \"source_file\": meta_item[\"source_file\"],\n",
        "        \"sentence_text\": meta_item[\"sentence_text\"],\n",
        "        \"orig_sentence\": meta_item[\"orig_sentence\"],\n",
        "        \"sentence\": tokens,\n",
        "        \"entities\": entities\n",
        "    })\n",
        "\n",
        "# Сохранение\n",
        "output_path = f\"{OUTPUTS_DIR}/validation_results.json\"\n",
        "with open(output_path, 'w', encoding='utf-8') as f:\n",
        "    json.dump(decoded_full, f, ensure_ascii=False, indent=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsXw0H9OUqyK"
      },
      "source": [
        "---\n",
        "## Statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yWdf1-w1pgxK"
      },
      "outputs": [],
      "source": [
        "print(f\"Предложений: {len(decoded_full):,}\")\n",
        "print(f\"Сущностей: {total_entities:,}\")\n",
        "\n",
        "type_counts = Counter()\n",
        "for rec in decoded_full:\n",
        "    for ent in rec.get('entities', []):\n",
        "        type_counts[ent.get('type', 'UNKNOWN')] += 1\n",
        "\n",
        "print(\"\\nПо типам:\")\n",
        "for etype, count in sorted(type_counts.items(), key=lambda x: -x[1]):\n",
        "    pct = count / total_entities * 100 if total_entities > 0 else 0\n",
        "    print(f\"  {etype:<35} {count:>5} ({pct:>5.1f}%)\")\n",
        "\n",
        "by_file = defaultdict(lambda: {\"sentences\": 0, \"entities\": 0})\n",
        "for rec in decoded_full:\n",
        "    filename = rec[\"source_file\"]\n",
        "    by_file[filename][\"sentences\"] += 1\n",
        "    by_file[filename][\"entities\"] += len(rec.get('entities', []))\n",
        "\n",
        "print(\"\\nПо файлам:\")\n",
        "for filename in sorted(by_file.keys()):\n",
        "    stats = by_file[filename]\n",
        "    avg = stats[\"entities\"] / stats[\"sentences\"] if stats[\"sentences\"] > 0 else 0\n",
        "    print(f\"  {filename:<50} │ {stats['entities']:>4} сущ. │ {avg:.2f}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZInyN53xsx5a"
      },
      "source": [
        "---\n",
        "## Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ЧТО РАБОТАЕТ:**\n",
        "\n",
        "Основные типы (ACTION, ADVERSE_EVENT, DRUG, AGE_GROUP, FOLLOW_UP) - 97% всех сущностей\n",
        "Модель стабильна на всех 9 файлах\n",
        "F1 ≈ 92-95% судя по результатам\n",
        "\n",
        "**ЧТО МОЖНО УЛУЧШИТЬ:**\n",
        "\n",
        "- PROCEDURE_TEST - только 28 (в train было 482)\n",
        "- LAB_VALUE - только 7 (в train было 110)\n",
        "- SEVERITY/STAGE - только 1 (в train было 66)\n",
        "\n",
        "ПРИЧИНА: Validation данные (про рак, лейкемию и др.) содержат другие типы сущностей, чем train данные (необходимо дальнейшее обучение)."
      ],
      "metadata": {
        "id": "hjYH62_XY4pS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "hIHhInRvZW9q"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}